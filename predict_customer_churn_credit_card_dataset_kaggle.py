# -*- coding: utf-8 -*-
"""Predict Customer Churn_Credit Card Dataset_Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/phamhuyen286/164eaf94b6b37586111295f2467aa865/predict-customer-churn_credit-card-dataset_kaggle.ipynb

A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction

I got this dataset from a website with the URL as https://leaps.analyttica.com/home. I have been using this for a while to get datasets and accordingly work on them to produce fruitful results. The site explains how to solve a particular business problem.

Now, this dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.

We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

path = '/content/'

data = pd.read_csv(path+'BankChurners.csv')

data.head(5)

data.dtypes

data.shape

# as the data suggested to delete two last columns so that it's easy to not making any confuse

data = data.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis=1)

data.shape

"""Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as Non-churning will do. So recall (TP/TP+FN) need to be higher."""

data.head(5)

# check for the missing value

data.isnull().sum(axis=0) # we dont have missing value in itit

"""As part of project requirement, we would like to study all features which most influential factors to customer churn. To do that we will perform EDA"""



"""## Univariate Analysis are using for categorical data"""

data.Attrition_Flag.unique() # we will use label encoder for this

data['Attrition_Flag'].value_counts(normalize =True)

"""Attrition is set of customer who is churn. we got 16% of customer under Attrition & 84% of them are existing"""

import seaborn as sns
import matplotlib.pyplot as plt
data.groupby(['Education_Level','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)
plt.show()

data.groupby(['Marital_Status','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)
plt.show()

data.groupby(['Card_Category','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)
plt.show()

data.groupby(['Income_Category','Attrition_Flag']).size().unstack().plot(kind='bar',stacked=True)
plt.show()

from sklearn.preprocessing import LabelEncoder
af_le = LabelEncoder()
data['Attrition_Flag'] = af_le.fit_transform(data['Attrition_Flag'])

data.Education_Level.unique() #Unknown can be a factor to look into other in the future. however we can check if we have Unknown of education level in the data

data['Education_Level'].value_counts()

data['Education_Level'].value_counts(normalize=True)

"""We understand that 6 defined categories for Education Level while most contribution come from Graduate for 31% and then Highschool. However there are number of Unknown also there. Let us check how many unknown in Education level has been associate with the Attrition and also other"""

data['Marital_Status'].value_counts(normalize=True)

data['Card_Category'].value_counts(normalize=True)

data['Gender'].value_counts(normalize=True)

#Encoding the categorical data

from sklearn.preprocessing import LabelEncoder

gender_le = LabelEncoder()
data['Gender'] = gender_le.fit_transform(data['Gender'])



Edu_le = LabelEncoder()
data['Education_Level'] = Edu_le.fit_transform(data['Education_Level'])

card_le = LabelEncoder()
data['Card_Category'] = card_le.fit_transform(data['Card_Category'])

marital_le = LabelEncoder()
data['Marital_Status'] = marital_le.fit_transform(data['Marital_Status'])

data.head(5)

data['Income_Category'].value_counts(normalize=True)

# the same we can also encode the data
income_le = LabelEncoder()
data['Income_Category'] = income_le.fit_transform(data['Income_Category'])

data.head(5)

data = data.drop(columns=['CLIENTNUM'])

data.dtypes

data.describe()

"""# Features Important"""

#Using Pearson Correlation
plt.figure(figsize=(22,20))
cor = data.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

# Split input & output data

y = data['Attrition_Flag']
y = np.array(y)
y = y.reshape(-1,1)
y.shape

X = data.drop(columns=['Attrition_Flag'])
X.shape

# Split train & test  data

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

#Import libraries for modelling

from scipy.stats import zscore
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix
from time import time
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier
lr=LogisticRegression()
dt=DecisionTreeClassifier()
knn=KNeighborsClassifier()
rf=RandomForestClassifier()
ada=AdaBoostClassifier()
bag=BaggingClassifier()
xtree=ExtraTreesClassifier()
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
count=CountVectorizer()
from sklearn.decomposition import PCA

X=X.apply(zscore)

start_time=time()
model_list=[lr,dt,knn,rf,ada,bag,xtree]
Score=[]
for i in model_list:
    x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)
    i.fit(x_train,y_train)
    y_pred=i.predict(x_test)
    score=accuracy_score(y_test,y_pred)
    Score.append(score)
print(pd.DataFrame(zip(model_list,Score),columns=['Model Used','R2-Score']))
end_time=time()
print(round(end_time-start_time,2),'sec')

"""Find out F1, accuracy and Recall for RandomForest"""

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)
rf=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=100)
rf.fit(x_train,y_train)
y_pred=rf.predict(x_test)
accuracy_score(y_test,y_pred)

train_pred = rf.predict(x_train)

test_pred = rf.predict(x_test)

# Use score method to get accuracy of model
score = rf.score(x_train, y_train)
print('Train F1 Score = {} %'.format(round(score,4)*100))

# Use score method to get accuracy of model
score = rf.score(x_test, y_test)
print('Test F1 Score = {} %'.format(round(score,4)*100))

from sklearn.metrics import confusion_matrix
import seaborn as sns

conf_matrix = confusion_matrix(y_test, test_pred)
print(conf_matrix)

plt.figure(figsize=(11,9))
sns.heatmap(conf_matrix, annot=True, fmt=".3f", linewidths=.5)

plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = 'Test F1 Score:{}%'.format(round(score, 4)*100)
plt.title(all_sample_title, size = 15)

from sklearn.metrics import classification_report

print('*********************  Training Data Report  **********')
print(classification_report(y_train, train_pred))

print('***********************  Test Data Report  **********')
print(classification_report(y_test, test_pred))

















